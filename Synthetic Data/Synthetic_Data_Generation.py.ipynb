{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e36a27d-af6c-478f-8d89-36b0b70eaa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install sdv pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffff59f-a587-45fd-8df2-2c30923abf43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Install necessary libraries for Synthetic Data Generation\n",
    "# Run this once in your cluster\n",
    "%pip install sdv plotly matplotlib seaborn pandas scikit-learn\n",
    "\n",
    "# COMMAND ----------\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# SDV (Synthetic Data Vault) Imports\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392906e2-18f9-498f-ad63-5680f3a8fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# --- STEP 1: LOAD DATA ---\n",
    "\n",
    "# NOTE: Update this path to where your file is stored in Databricks\n",
    "# Common path structure: \"/dbfs/FileStore/tables/Online_Sales_Data.csv\"\n",
    "# or if using Unity Catalog Volumes: \"/Volumes/catalog/schema/volume/Online_Sales_Data.csv\"\n",
    "file_path = \"/Workspace/Users/bchandrach@gmail.com/AdvDataAnalytics-Project-/Online Sales Data.csv\"\n",
    "\n",
    "try:\n",
    "    # Try reading from the DBFS path\n",
    "    real = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path}. Please check the path.\")\n",
    "    # Fallback for demonstration if file isn't found (creates a dummy frame or stops)\n",
    "    raise\n",
    "\n",
    "print(\"Real data loaded:\", real.shape)\n",
    "display(real.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542f07b1-c695-471e-af8f-89f6766e333d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 2: PREPROCESSING ---\n",
    "\n",
    "# Convert Date to datetime for better handling\n",
    "real['Date'] = pd.to_datetime(real['Date'])\n",
    "\n",
    "# Separate columns that shouldn't be synthesized directly\n",
    "# 1. Transaction ID is a key, we will regenerate it later.\n",
    "# 2. Total Revenue is derived (Units * Price), we will recalculate it to ensure consistency.\n",
    "training_data = real.drop(columns=['Transaction ID', 'Total Revenue'])\n",
    "\n",
    "print(\"Training data prepared. Columns:\", list(training_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2555c9-7aac-4aa6-8e67-fadee919c4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 3: SETUP SYNTHESIZER ---\n",
    "\n",
    "# Detect metadata from the dataframe\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data=training_data)\n",
    "\n",
    "print(\"\\nMetadata detected:\")\n",
    "print(metadata.to_dict())\n",
    "\n",
    "# Initialize Gaussian Copula Synthesizer (Fast and effective for this size)\n",
    "# You can switch to CTGANSynthesizer for more complex distributions if needed\n",
    "synthesizer = GaussianCopulaSynthesizer(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72a1fd9-0f20-43b9-87fa-4fac4c259b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 4: TRAIN AND SAMPLE ---\n",
    "\n",
    "print(\"\\nFitting synthesizer...\")\n",
    "synthesizer.fit(training_data)\n",
    "\n",
    "print(\"\\nGenerating synthetic data...\")\n",
    "# Generate the same number of rows as the real dataset\n",
    "synth_data = synthesizer.sample(num_rows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c8194c-54c3-4bc4-8beb-3a625d2377e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 5: POST-PROCESSING ---\n",
    "\n",
    "# 1. Recalculate Total Revenue to maintain mathematical consistency\n",
    "synth_data['Total Revenue'] = synth_data['Units Sold'] * synth_data['Unit Price']\n",
    "\n",
    "# 2. Regenerate Transaction ID (continuing from the original or starting fresh)\n",
    "# Here we recreate IDs similar to the original format\n",
    "start_id = real['Transaction ID'].min()\n",
    "synth_data['Transaction ID'] = range(start_id, start_id + len(synth_data))\n",
    "\n",
    "# Reorder columns to match original\n",
    "synth_data = synth_data[real.columns]\n",
    "\n",
    "# Display results\n",
    "print(\"Synthetic data generated:\", synth_data.shape)\n",
    "display(synth_data.head())\n",
    "\n",
    "# Save to DBFS (Optional)\n",
    "output_path = \"/Workspace/Users/bchandrach@gmail.com/AdvDataAnalytics-Project-/Synthetic_Online_Sales.csv\"\n",
    "synth_data.to_csv(output_path, index=False)\n",
    "print(f\"Synthetic data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db8358f-570f-42c5-835c-15467a6eb23b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 6: EVALUATION (PCA & METRICS) ---\n",
    "\n",
    "def visualize_pca(real_df, synth_df, label):\n",
    "    # Select numeric columns for PCA (exclude IDs)\n",
    "    numeric_cols = ['Units Sold', 'Unit Price', 'Total Revenue']\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_real = scaler.fit_transform(real_df[numeric_cols])\n",
    "    X_synth = scaler.transform(synth_df[numeric_cols])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pcs_real = pca.fit_transform(X_real)\n",
    "    pcs_synth = pca.transform(X_synth)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(pcs_real[:,0], pcs_real[:,1], alpha=0.5, label=\"Real\", c='blue', s=20)\n",
    "    plt.scatter(pcs_synth[:,0], pcs_synth[:,1], alpha=0.5, label=\"Synthetic\", c='orange', s=20)\n",
    "    plt.title(f\"PCA â€” Real vs {label}\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run PCA Visualization\n",
    "visualize_pca(real, synth_data, \"Gaussian Copula Synthetic\")\n",
    "\n",
    "# Compare Statistics\n",
    "print(\"\\n--- Summary Statistics Comparison ---\")\n",
    "comp_stats = pd.concat([real.describe(), synth_data.describe()], axis=1, keys=['Real', 'Synthetic'])\n",
    "print(comp_stats)\n",
    "\n",
    "# Detectability (Silhouette Score)\n",
    "def detectability_score(real_df, synth_df):\n",
    "    numeric_cols = ['Units Sold', 'Unit Price', 'Total Revenue']\n",
    "    \n",
    "    combined = pd.concat([real_df[numeric_cols], synth_df[numeric_cols]])\n",
    "    X = StandardScaler().fit_transform(combined)\n",
    "    \n",
    "    # If the data is distinguishable, KMeans will cluster them apart\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # A lower score usually indicates better blending (harder to distinguish)\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "score = detectability_score(real, synth_data)\n",
    "print(f\"\\nDetectability Silhouette Score: {score:.4f} (Lower is often better for privacy/blending)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a433ff89-f1e6-457d-a151-4c5f6fd4b4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Calculate Mean Absolute Correlation Difference ---\n",
    "\n",
    "# 1. Select numeric columns for correlation analysis\n",
    "# We exclude 'Transaction ID' as it's just a key, and 'Date' requires different handling\n",
    "numeric_cols = ['Units Sold', 'Unit Price', 'Total Revenue']\n",
    "\n",
    "# 2. Compute Correlation Matrices\n",
    "real_corr = real[numeric_cols].corr()\n",
    "synth_corr = synth_data[numeric_cols].corr()\n",
    "\n",
    "# 3. Calculate Mean Absolute Difference\n",
    "# This takes the absolute difference between every cell in the matrices and averages them\n",
    "correlation_difference = (real_corr - synth_corr).abs().mean().mean()\n",
    "\n",
    "print(f\"Mean Absolute Correlation Difference: {correlation_difference:.4f}\")\n",
    "\n",
    "# --- (Optional) Visualize the Difference ---\n",
    "# Plotting the difference matrix helps you see WHICH correlations are off\n",
    "diff_matrix = (real_corr - synth_corr).abs()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(diff_matrix, annot=True, cmap='Reds', vmin=0, fmt=\".3f\")\n",
    "plt.title(\"Absolute Correlation Difference (Real - Synthetic)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Synthetic_Data_Generation.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}